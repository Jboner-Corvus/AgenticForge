<p align="center">
  <img src="assets/title.png" alt="G-Forge Logo" width="250">
</p>

<h1 align="center">G-Forge</h1>

<p align="center">
  <img src="https://img.shields.io/badge/üî®-G_Forge-orange?style=for-the-badge" alt="G-Forge Logo">
</p>
<p align="center">
  <strong>üåê Langues disponibles</strong><br>
  <a href="README_EN.md">English</a> ‚Ä¢ 
  <a href="README.md">Fran√ßais</a> ‚Ä¢ 
  <a href="README_CHS.md">‰∏≠Êñá</a> ‚Ä¢ 
  <a href="README_CHT.md">ÁπÅÈ´î‰∏≠Êñá</a> ‚Ä¢ 
  <a href="README_JP.md">Êó•Êú¨Ë™û</a> ‚Ä¢ 
  <a href="README_PTBR.md">Portugu√™s (Brasil)</a> ‚Ä¢ 
  <a href="README_ES.md">Espa√±ol</a>
</p> 
<h3 align="center">
      Una alternativa privada y local a MANUS.
</h3>

<p align="center">
  <em>
    Un agente de IA 100% aut√≥nomo, gratuito y local que forja sus propias herramientas, escribe c√≥digo y ejecuta tareas complejas, manteniendo todos los datos en su dispositivo. Basado en el protocolo MCP (Model Context Protocol) con FastMCP como motor, est√° dise√±ado para modelos de razonamiento locales y es adaptable a la API de su LLM favorito, garantizando privacidad total y sin dependencias de la nube.
  </em>
</p>
<br>
<p align="center">
    <img src="https://img.shields.io/badge/License-MIT-green.svg?style=flat-square&logo=opensource&logoColor=white" alt="MIT License"> <img src="https://img.shields.io/github/stars/Jboner-Corvus/AgenticForge?style=flat-square&logo=github&color=gold" alt="Stars"> <img src="https://img.shields.io/github/forks/Jboner-Corvus/AgenticForge?style=flat-square&logo=git&color=blue" alt="Forks"> <img src="https://img.shields.io/github/issues/Jboner-Corvus/AgenticForge?style=flat-square&logo=github" alt="Issues">
</p>
<p align="center">
    <img src="https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white" alt="Docker">
    <img src="https://img.shields.io/badge/Node.js-339933?style=for-the-badge&logo=nodedotjs&logoColor=white" alt="Node.js">
    <img src="https://img.shields.io/badge/TypeScript-3178C6?style=for-the-badge&logo=typescript&logoColor=white" alt="TypeScript">
    <img src="https://img.shields.io/badge/Redis-DC382D?style=for-the-badge&logo=redis&logoColor=white" alt="Redis">
    <img src="https://img.shields.io/badge/MCP-000000?style=for-the-badge&logoColor=white" alt="MCP">
    <img src="https://img.shields.io/badge/pnpm-F69220?style=for-the-badge&logo=pnpm&logoColor=white" alt="pnpm">
</p>

## ¬øPor qu√© G-Forge?

üîí **Completamente Local y Privado** - Todo funciona en su m√°quina ‚Äî sin nube, sin compartir datos. Sus archivos, conversaciones y herramientas permanecen privados.

üõ†Ô∏è **Auto-Forjado de Herramientas** - G-Forge puede crear sus propias herramientas ‚Äî cuando le falta una capacidad, escribe el c√≥digo para construirla.

üíª **Asistente de Codificaci√≥n Aut√≥nomo** - ¬øNecesita c√≥digo? Puede escribir, depurar y ejecutar programas en Python, TypeScript, Bash y m√°s ‚Äî sin supervisi√≥n.

üß† **Selecci√≥n Inteligente de Herramientas** - Usted pregunta, autom√°ticamente encuentra la mejor herramienta para el trabajo. Como tener una forja de expertos listos para ayudar.

üìã **Planifica y Ejecuta Tareas Complejas** - Desde gesti√≥n de archivos hasta web scraping ‚Äî puede dividir grandes tareas en pasos y forjar las herramientas para realizar el trabajo.

üåê **Navegaci√≥n Web Inteligente** - G-Forge puede navegar por internet de forma aut√≥noma ‚Äî buscar, leer, extraer informaci√≥n, automatizar tareas ‚Äî todo sin intervenci√≥n.

üöÄ **Impulsado por FastMCP** - Utiliza el protocolo MCP (Model Context Protocol) con FastMCP como framework ultra-performante ‚Äî un verdadero cohete para interacciones LLM.

---

## Demo

> **"¬øPuedes crear una herramienta para analizar mis archivos CSV y luego usarla para generar un informe de sales_data.csv?"**

---

## üõ†Ô∏è ‚ö†Ô∏è Trabajo Activo en Progreso

üôè Este proyecto comenz√≥ para demostrar que MCP era mejor que API y ha crecido m√°s all√° de las expectativas. Contribuciones, comentarios y paciencia son profundamente apreciados mientras forjamos hacia adelante.

---

## Prerrequisitos

Antes de comenzar, aseg√∫rese de tener el siguiente software instalado:

- **Git**: Para clonar el repositorio. [Descargar Git](https://git-scm.com/)
- **Docker Engine & Docker Compose**: Para ejecutar los servicios agrupados.
  - [Instalar Docker Desktop](https://www.docker.com/products/docker-desktop/) (incluye Docker Compose V2): Windows | Mac | Linux
  - O instalar por separado: [Docker Engine](https://docs.docker.com/engine/install/) | [Docker Compose](https://docs.docker.com/compose/install/)
- **Node.js 20+**: Para la interfaz web. [Descargar Node.js](https://nodejs.org/)
- **pnpm**: Gestor de paquetes. Instalar con `npm install -g pnpm`

---

## 1. Clonar el repositorio

```bash
git clone https://github.com/your-username/agentic-forge.git
cd agentic-forge
```

## 2. Ejecutar el script de instalaci√≥n

Haga ejecutable el script de gesti√≥n y ejec√∫telo.

```bash
chmod +x run.sh
./run.sh
```

En la primera ejecuci√≥n, el script verificar√° si existe un archivo `.env`. Si no existe, lo crear√° autom√°ticamente para usted.

## 3. Configurar su entorno

Una vez que se crea el archivo `.env`, √°bralo y complete los valores con sus propias credenciales.

```env
# Copie este archivo a .env y complete los valores.
HOST_PORT=8080
PORT=8080
NODE_ENV=development
LOG_LEVEL=info
AUTH_TOKEN=""
REDIS_HOST=redis
REDIS_PORT=6378
REDIS_HOST_PORT=6378
REDIS_PASSWORD=""
# La URL base ya no es necesaria para la API de Google, comente o elim√≠nela.
# LLM_API_BASE_URL=
WEB_PORT=3000
# Use su clave de API de Google Gemini
LLM_API_KEY=""

# Especifique un modelo Gemini, ej: "gemini-1.5-pro-latest"
LLM_MODEL_NAME=gemini-2.5-flash
```

**Importante**:

- Establezca un `AUTH_TOKEN` fuerte (se recomiendan 32+ caracteres)
- Las claves API son opcionales si usa modelos locales

---

## 4. Iniciar Docker

Aseg√∫rese de que Docker est√© ejecut√°ndose antes de continuar.

---

## Configuraci√≥n LLM Local (Recomendado)

### Requisitos de Hardware

| Tama√±o del Modelo | Memoria GPU | Rendimiento                            |
| ----------------- | ----------- | -------------------------------------- |
| 7B                | 8GB VRAM    | ‚ö†Ô∏è Solo tareas b√°sicas                 |
| 14B               | 12GB VRAM   | ‚úÖ La mayor√≠a de tareas funcionan bien |
| 32B               | 24GB VRAM   | üöÄ Excelente rendimiento               |
| 70B+              | 48GB+ VRAM  | üí™ Calidad profesional                 |

### Configuraci√≥n con Ollama (Recomendado)

1.  **Instalar Ollama**: [Descargar Ollama](https://ollama.ai/)
2.  **Iniciar Ollama**:
    ```bash
    ollama serve
    ```
3.  **Descargar un modelo de razonamiento**:
    ```bash
    ollama pull deepseek-r1:14b
    # o para m√°s potencia: ollama pull deepseek-r1:32b
    ```
4.  **Actualizar configuraci√≥n** en `.env`:
    ```env
    LLM_MODEL_NAME="deepseek-r1:14b"
    LLM_API_BASE_URL="http://localhost:11434"
    ```

### Alternativa: LM Studio

1.  Descargue e instale [LM Studio](https://lmstudio.ai/)
2.  Cargue un modelo como `deepseek-r1-distill-qwen-14b`
3.  Inicie el servidor local
4.  Actualice `.env`:
    ```env
    LLM_API_BASE_URL="http://localhost:1234"
    ```

---

## Configuraci√≥n para Uso de API

Si prefiere modelos en la nube o carece de hardware suficiente:

### 1. Elegir un Proveedor de API

| Proveedor | Ejemplos de Modelos                  | Enlace de Clave API                                       |
| --------- | ------------------------------------ | --------------------------------------------------------- |
| OpenAI    | `gpt-4`, `o1`                        | [platform.openai.com](https://platform.openai.com/signup) |
| Google    | `gemini-2.5-pro`, `gemini-2.5-flash` | [aistudio.google.com](https://aistudio.google.com/keys)   |
| Anthropic | `claude-4-sonnet`, `claude-4-opus`   | [console.anthropic.com](https://console.anthropic.com/)   |
| DeepSeek  | `deepseek-chat`, `deepseek-coder`    | [platform.deepseek.com](https://platform.deepseek.com)    |

### 2. Establecer su clave API

**Linux/macOS:**

```bash
export LLM_API_KEY="your_api_key_here"
# Agregue a ~/.bashrc o ~/.zshrc para persistencia
```

**Windows:**

```cmd
set LLM_API_KEY=your_api_key_here
```

### 3. Actualizar `.env`:

```env
LLM_API_KEY="your_api_key_here"
LLM_MODEL_NAME="gemini-1.5-pro"
```

---

## Iniciar Servicios y Ejecutar

### Usando la Consola de Gesti√≥n (`run.sh`)

Despu√©s de configurar su archivo `.env`, use la consola de gesti√≥n para iniciar la aplicaci√≥n.

Lance la consola interactiva:

```bash
./run.sh
```

Desde el men√∫ de la consola:

1.  **Iniciar** - Lanzar todos los servicios
2.  **Estado** - Verificar la salud de los servicios
3.  **Logs** - Monitorear logs en tiempo real

### Comandos Docker Manuales

Iniciar todos los servicios:

```bash
docker-compose up -d
```

Verificar estado:

```bash
docker-compose ps
```

Ver logs:

```bash
docker-compose logs -f
```

**‚ö†Ô∏è Advertencia**: El inicio inicial puede tomar 10-15 minutos ya que se descargan las im√°genes Docker y se inicializan los servicios. Espere a ver `backend: "GET /health HTTP/1.1" 200 OK` en los logs.

---

## Puntos de Acceso

Una vez que los servicios est√°n ejecut√°ndose:

| Servicio                  | URL                                       | Descripci√≥n                      |
| ------------------------- | ----------------------------------------- | -------------------------------- |
| **Interfaz Web**          | http://localhost:3000                     | Interfaz principal del usuario   |
| **Endpoint de API**       | http://localhost:8080/api/v1/agent/stream | Acceso directo a la API          |
| **Verificaci√≥n de Salud** | http://localhost:8080/health              | Estado de salud de los servicios |

### Prueba R√°pida

```bash
# Verificaci√≥n de salud
curl http://localhost:8080/health

# Prueba de API
curl -X POST http://localhost:8080/api/v1/agent/stream \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_AUTH_TOKEN" \
  -d '{"goal": "Crear un script Python hello world simple"}'
```

---

## Ejemplos de Uso

Una vez que sus servicios est√°n
